





import gymnasium as gym
import matplotlib.pyplot as plt
import numpy as np
import random
import os
import custom_envs
from render_util import visualize, plot_action_value
from matplotlib.animation import FuncAnimation
%matplotlib inline
from IPython.display import Video
from IPython.display import display
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning) 








class Agent():
    def __init__(self, env, gamma=1.0, learning_rate=0.05, epsilon=0.1):
        """ Initializes the environment and defines dynamics.
        
        Please DON'T change the names of the variables that are already defined in this method.
        The Q-functions shall be called 'self.q_1' and 'self.q_2', besides, there exist 'self.env', 'self.learning_rate', 
        'self.gamma' and 'self.epsilon'
        """
        self.env = env
        self.gamma = gamma
        self.learning_rate = learning_rate
        self.epsilon = epsilon 
            

        self.q_1 = np.zeros((self.env.observation_space.n, self.env.action_space.n))
        self.q_2 = np.zeros((self.env.observation_space.n, self.env.action_space.n))

    # YOUR CODE HERE
    # raise NotImplementedError()





# Do we have both Q-Functions?

map = ["SFFH", "FFFH", "HFFH", "HFFG"]
test_env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) 
test_agent = Agent(test_env, gamma=1.0, learning_rate=0.05, epsilon=0.1)
assert test_agent.q_1[0][0] != None
assert test_agent.q_2[0][0] != None


# Used for grading. Do not change.





def get_best_action(self, obs, q):
    """ Return the best action based on the Q-function.
    
    Args: 
        obs: state of the environment
        q: The chosen Q-Function, a numpy array of shape (num_states, num_actions)
    Returns:
        best_action: Chosen action
    """
    
    best_action = np.random.choice(np.flatnonzero(np.isclose(q[obs], (q[obs]).max(), rtol=0.01)))
    return best_action

def epsilon_greedy_policy(self, obs, q):
    """ Return an action based on the Q-function and probability self.epsilon.
    
    The action should be random with probability self.epsilon, or otherwise the best action based on the Q-function.
    
    Args: 
        obs: state of the environment
        q: The chosen Q-Function, a numpy array of shape (num_states, num_actions)
    Returns:
        action: Chosen action
    """
    # 생성된 값이 앱실론 0.5보다 작으면 임의의 행동을 선택 (이전에 경험하지 못한 행동을 찾는다)
    # 생성된 값이 앱실론 0.5보다 크면 가장 좋은 가치를 갖는 행동을 선택
    if np.random.random() < self.epsilon:
        return np.random.choice(range(self.env.action_space.n))
    else:
        return self.get_best_action(obs, q)

    
# YOUR CODE HERE
raise NotImplementedError()


setattr(Agent, 'get_best_action', get_best_action)
setattr(Agent, 'epsilon_greedy_policy', epsilon_greedy_policy)





# Can we get an action?

test_env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) 
test_agent = Agent(test_env, epsilon = 0.0)
test_agent.q_1[0][0] = 1
assert test_agent.epsilon_greedy_policy(0, test_agent.q_1) == 0


# Used for grading. Do not change.





def train(self, num_episodes):
    """ Trains the agent with the double-q algorithm.
    
    Args: 
        num_episodes: Number of episodes used until training stops
    """
    for i in range(num_episodes+1):
        obs, info = self.env.reset()
        done = False
        while not done:
            # In this implementation we only use n=1, but we could extend it for n = ... using a numpy array
            # Choose action and perform step
            action = self.epsilon_greedy_policy(obs, self.q_1 + self.q_2)
            next_obs, reward, done, truncated, info = self.env.step(action)
            # TD Update
            if np.random.rand() <= 0.5:
                best_next_action_q1 = self.get_best_action(next_obs, self.q_1)
                td_target = reward + self.gamma * self.q_2[next_obs][best_next_action_q1]
                update = (1-self.learning_rate) * self.q_1[obs][action] + self.learning_rate * td_target
                self.q_1[obs][action] = update

            else:
                # If Update(B)
                best_next_action_q2 = self.get_best_action(next_obs, self.q_2)
                td_target = reward + self.gamma * self.q_1[next_obs][best_next_action_q2]
                update = (1-self.learning_rate) * self.q_2[obs][action] + self.learning_rate * td_target
                self.q_2[obs][action] = update
                
            obs = next_obs


# YOUR CODE HERE
# raise NotImplementedError()

            # obs = next_obs


setattr(Agent, 'train', train)
setattr(Agent, 'visualize', visualize)
setattr(Agent, 'plot_action_value', plot_action_value)





# Does our policy change?

map = ["SFFH", "FFFH", "HFFH", "HFFG"]
test_env = gym.make('CustomFrozenLake-v1', render_mode=None, desc=map, is_slippery=False) 
test_agent = Agent(test_env)
test_agent.visualize(0)
test_agent.train(100)
test_agent.visualize(100)


# Used for grading. Do not change.





def evaluate(self, env, file, num_runs=5):
    """ Evaluates the agent in the environment.

    Args:
        env: Environment we want to use. 
        file: File used for storing the video.
        num_runs: Number of runs displayed
    Returns:
        done: Info about whether the last run is done.
        reward: The reward the agent gathered in the last step.
    """
    frames = []  # collect rgb_image of agent env interaction
    video_created = False
    for _ in range(num_runs):
        done = False
        obs, info = env.reset()
        out = env.render()
        frames.append(out)
        while not done:
            action = self.get_best_action(obs, self.q_1 + self.q_2)
            obs, reward, done, truncated, info = env.step(action)
            
            
# YOUR CODE HERE
# raise NotImplementedError()

            out = env.render()
            frames.append(out)
                
    # create animation out of saved frames
    if all(frame is not None for frame in frames):
        fig = plt.figure(figsize=(10, 6))
        plt.axis('off')
        img = plt.imshow(frames[0])
        def animate(index):
            img.set_data(frames[index])
            return [img]
        anim = FuncAnimation(fig, animate, frames=len(frames), interval=20)
        plt.close()
        anim.save(file, writer="ffmpeg", fps=5)
        
    return done, reward

setattr(Agent, 'evaluate', evaluate)





# Can we run evaluate?

map = ["SFFH", "FFGH", "HFFH", "HFFF"]
test_env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False) 
test_agent = Agent(test_env)
test_agent.q_1[0][2] = 1
test_agent.q_1[1][2] = 1
test_agent.q_1[2][2] = 1
# This policy leads the agent to an ice-hole on the right, for a video check the file "test_run.mp4"

test_video = "test_run.mp4"
test_run = test_agent.evaluate(test_env, test_video)[0] 
assert test_run == True


# Used for grading. Do not change.





training_runs = 10000
map = ["SFFF", "FHFH", "FHFH", "FFFG"]
env = gym.make('CustomFrozenLake-v1', render_mode='rgb_array', desc=map, is_slippery=False) 
env.reset()
final_agent = Agent(env, gamma=0.9)
final_agent.train(training_runs)
final_agent.visualize(training_runs)
video = "final_run.mp4"
final_agent.evaluate(env, video, num_runs=1)
Video(video, html_attributes="loop autoplay")


# This is the end of task 2, please proceed with task 3



